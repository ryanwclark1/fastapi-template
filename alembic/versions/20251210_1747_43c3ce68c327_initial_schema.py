"""initial schema

Revision ID: 43c3ce68c327
Revises:
Create Date: 2025-12-10 17:47:34.711865+00:00

"""

from __future__ import annotations

from collections.abc import Sequence

import sqlalchemy as sa
from alembic import op
from example_service.core.database.search.types import TSVECTOR
from example_service.core.database.types import EncryptedString
from example_service.features.webhooks.models import StringArray
from sqlalchemy import Text
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = "43c3ce68c327"
down_revision: str | None = None
branch_labels: str | Sequence[str] | None = None
depends_on: str | Sequence[str] | None = None


def upgrade() -> None:  # noqa: PLR0915
    """Upgrade database schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "audit_logs",
        sa.Column(
            "timestamp",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="When the action occurred",
        ),
        sa.Column(
            "action",
            postgresql.ENUM(
                "create",
                "read",
                "update",
                "delete",
                "bulk_create",
                "bulk_update",
                "bulk_delete",
                "export",
                "import",
                "login",
                "logout",
                "login_failed",
                "password_change",
                "token_refresh",
                "permission_denied",
                "acl_check",
                "archive",
                "restore",
                "purge",
                name="auditaction",
            ),
            nullable=False,
            comment="Type of action performed",
        ),
        sa.Column(
            "entity_type",
            sa.String(length=100),
            nullable=False,
            comment="Type of entity affected",
        ),
        sa.Column(
            "entity_id",
            sa.String(length=255),
            nullable=True,
            comment="ID of the affected entity",
        ),
        sa.Column(
            "user_id",
            sa.String(length=255),
            nullable=True,
            comment="User who performed the action",
        ),
        sa.Column(
            "actor_roles",
            postgresql.JSONB(astext_type=Text()),
            server_default="[]",
            nullable=False,
            comment="Roles the user had at time of action (for compliance audits)",
        ),
        sa.Column(
            "tenant_id", sa.String(length=255), nullable=True, comment="Tenant context"
        ),
        sa.Column(
            "old_values",
            postgresql.JSONB(astext_type=Text()),
            nullable=True,
            comment="Previous state (for updates/deletes)",
        ),
        sa.Column(
            "new_values",
            postgresql.JSONB(astext_type=Text()),
            nullable=True,
            comment="New state (for creates/updates)",
        ),
        sa.Column(
            "changes",
            postgresql.JSONB(astext_type=Text()),
            nullable=True,
            comment="Changed fields with old/new values",
        ),
        sa.Column(
            "ip_address",
            sa.String(length=45),
            nullable=True,
            comment="Client IP address",
        ),
        sa.Column(
            "user_agent",
            sa.String(length=500),
            nullable=True,
            comment="Client user agent",
        ),
        sa.Column(
            "request_id",
            sa.String(length=100),
            nullable=True,
            comment="Request correlation ID",
        ),
        sa.Column(
            "endpoint",
            sa.String(length=255),
            nullable=True,
            comment="API endpoint path",
        ),
        sa.Column("method", sa.String(length=10), nullable=True, comment="HTTP method"),
        sa.Column(
            "metadata",
            postgresql.JSONB(astext_type=Text()),
            nullable=True,
            comment="Additional context data",
        ),
        sa.Column(
            "success",
            sa.Boolean(),
            nullable=False,
            comment="Whether the action succeeded",
        ),
        sa.Column(
            "error_message",
            sa.Text(),
            nullable=True,
            comment="Error details if action failed",
        ),
        sa.Column(
            "duration_ms",
            sa.Integer(),
            nullable=True,
            comment="Action duration in milliseconds",
        ),
        sa.Column(
            "id",
            sa.Uuid(),
            nullable=False,
            comment="UUID v7 primary key (time-sortable)",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_audit_logs")),
    )
    with op.batch_alter_table("audit_logs", schema=None) as batch_op:
        batch_op.create_index(
            "ix_audit_action_entity", ["action", "entity_type"], unique=False
        )
        batch_op.create_index(
            "ix_audit_action_time", ["action", "timestamp"], unique=False
        )
        batch_op.create_index(
            "ix_audit_entity", ["entity_type", "entity_id"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_audit_logs_action"), ["action"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_audit_logs_entity_id"), ["entity_id"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_audit_logs_entity_type"), ["entity_type"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_audit_logs_request_id"), ["request_id"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_audit_logs_tenant_id"), ["tenant_id"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_audit_logs_timestamp"), ["timestamp"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_audit_logs_user_id"), ["user_id"], unique=False
        )
        batch_op.create_index(
            "ix_audit_tenant_time", ["tenant_id", "timestamp"], unique=False
        )
        batch_op.create_index(
            "ix_audit_tenant_user_time",
            ["tenant_id", "user_id", "timestamp"],
            unique=False,
        )
        batch_op.create_index(
            "ix_audit_user_time", ["user_id", "timestamp"], unique=False
        )

    op.create_table(
        "event_outbox",
        sa.Column(
            "event_type",
            sa.String(length=100),
            nullable=False,
            comment="Event type identifier",
        ),
        sa.Column(
            "event_version",
            sa.Integer(),
            nullable=False,
            comment="Event schema version",
        ),
        sa.Column(
            "payload", sa.Text(), nullable=False, comment="JSON-serialized event data"
        ),
        sa.Column(
            "correlation_id",
            sa.String(length=36),
            nullable=True,
            comment="Distributed tracing correlation ID",
        ),
        sa.Column(
            "aggregate_type",
            sa.String(length=100),
            nullable=True,
            comment="Aggregate type (e.g., User, Order)",
        ),
        sa.Column(
            "aggregate_id",
            sa.String(length=100),
            nullable=True,
            comment="Aggregate ID for partitioned processing",
        ),
        sa.Column(
            "processed_at",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="When the event was successfully published",
        ),
        sa.Column(
            "retry_count",
            sa.Integer(),
            nullable=False,
            comment="Number of failed publish attempts",
        ),
        sa.Column(
            "error_message",
            sa.Text(),
            nullable=True,
            comment="Last error message if publishing failed",
        ),
        sa.Column(
            "next_retry_at",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="Scheduled time for next retry attempt",
        ),
        sa.Column(
            "id",
            sa.Uuid(),
            nullable=False,
            comment="UUID v7 primary key (time-sortable)",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_event_outbox")),
    )
    with op.batch_alter_table("event_outbox", schema=None) as batch_op:
        batch_op.create_index(
            "ix_event_outbox_aggregate",
            ["aggregate_type", "aggregate_id", "created_at"],
            unique=False,
        )
        batch_op.create_index(
            batch_op.f("ix_event_outbox_correlation_id"),
            ["correlation_id"],
            unique=False,
        )
        batch_op.create_index(
            batch_op.f("ix_event_outbox_event_type"), ["event_type"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_event_outbox_next_retry_at"), ["next_retry_at"], unique=False
        )
        batch_op.create_index(
            "ix_event_outbox_pending",
            ["processed_at", "next_retry_at", "created_at"],
            unique=False,
            postgresql_where=sa.text("processed_at IS NULL"),
        )
        batch_op.create_index(
            batch_op.f("ix_event_outbox_processed_at"), ["processed_at"], unique=False
        )

    op.create_table(
        "feature_flags",
        sa.Column(
            "key", sa.String(length=100), nullable=False, comment="Unique flag key"
        ),
        sa.Column(
            "name", sa.String(length=200), nullable=False, comment="Human-readable name"
        ),
        sa.Column("description", sa.Text(), nullable=True, comment="Flag description"),
        sa.Column(
            "status",
            postgresql.ENUM(
                "enabled", "disabled", "percentage", "targeted", name="flagstatus"
            ),
            nullable=False,
            comment="Flag status",
        ),
        sa.Column(
            "enabled", sa.Boolean(), nullable=False, comment="Global enabled state"
        ),
        sa.Column(
            "percentage",
            sa.Integer(),
            nullable=False,
            comment="Rollout percentage (0-100)",
        ),
        sa.Column(
            "targeting_rules",
            postgresql.JSONB(astext_type=Text()),
            nullable=True,
            comment="Targeting rules for selective rollout",
        ),
        sa.Column(
            "metadata",
            postgresql.JSONB(astext_type=Text()),
            nullable=True,
            comment="Additional flag metadata",
        ),
        sa.Column(
            "starts_at",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="Activation start time",
        ),
        sa.Column(
            "ends_at",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="Activation end time",
        ),
        sa.Column(
            "id",
            sa.Uuid(),
            nullable=False,
            comment="UUID v7 primary key (time-sortable)",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_feature_flags")),
    )
    with op.batch_alter_table("feature_flags", schema=None) as batch_op:
        batch_op.create_index("ix_feature_flags_enabled", ["enabled"], unique=False)
        batch_op.create_index(batch_op.f("ix_feature_flags_key"), ["key"], unique=True)
        batch_op.create_index("ix_feature_flags_status", ["status"], unique=False)

    op.create_table(
        "files",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("original_filename", sa.String(length=255), nullable=False),
        sa.Column("storage_key", sa.String(length=500), nullable=False),
        sa.Column("bucket", sa.String(length=63), nullable=False),
        sa.Column("content_type", sa.String(length=127), nullable=False),
        sa.Column("size_bytes", sa.Integer(), nullable=False),
        sa.Column("checksum_sha256", sa.String(length=64), nullable=True),
        sa.Column("etag", sa.String(length=255), nullable=True),
        sa.Column(
            "status",
            postgresql.ENUM(
                "pending", "processing", "ready", "failed", "deleted", name="filestatus"
            ),
            nullable=False,
        ),
        sa.Column("owner_id", sa.String(length=255), nullable=True),
        sa.Column("is_public", sa.Boolean(), nullable=False),
        sa.Column("expires_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.Column(
            "tenant_id",
            sa.String(length=255),
            nullable=True,
            comment="Tenant ID for multi-tenant isolation",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_files")),
    )
    with op.batch_alter_table("files", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_files_owner_id"), ["owner_id"], unique=False
        )
        batch_op.create_index(batch_op.f("ix_files_status"), ["status"], unique=False)
        batch_op.create_index(
            batch_op.f("ix_files_storage_key"), ["storage_key"], unique=True
        )
        batch_op.create_index(
            batch_op.f("ix_files_tenant_id"), ["tenant_id"], unique=False
        )

    op.create_table(
        "flag_overrides",
        sa.Column(
            "flag_key",
            sa.String(length=100),
            nullable=False,
            comment="Feature flag key",
        ),
        sa.Column(
            "entity_type",
            sa.String(length=20),
            nullable=False,
            comment="Entity type (user, tenant)",
        ),
        sa.Column(
            "entity_id", sa.String(length=100), nullable=False, comment="Entity ID"
        ),
        sa.Column("enabled", sa.Boolean(), nullable=False, comment="Override value"),
        sa.Column("reason", sa.Text(), nullable=True, comment="Reason for override"),
        sa.Column(
            "id",
            sa.Uuid(),
            nullable=False,
            comment="UUID v7 primary key (time-sortable)",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_flag_overrides")),
    )
    with op.batch_alter_table("flag_overrides", schema=None) as batch_op:
        batch_op.create_index(
            "ix_flag_overrides_entity", ["entity_type", "entity_id"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_flag_overrides_flag_key"), ["flag_key"], unique=False
        )
        batch_op.create_index(
            "ix_flag_overrides_lookup",
            ["flag_key", "entity_type", "entity_id"],
            unique=True,
        )

    op.create_table(
        "jobs",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("tenant_id", sa.String(length=255), nullable=False),
        sa.Column(
            "task_name",
            sa.String(length=255),
            nullable=False,
            comment="Name of the task function",
        ),
        sa.Column(
            "task_args",
            postgresql.JSONB(astext_type=Text()),
            nullable=False,
            comment="Task arguments as JSON",
        ),
        sa.Column(
            "status",
            postgresql.ENUM(
                "pending",
                "queued",
                "running",
                "completed",
                "failed",
                "cancelled",
                "retrying",
                "paused",
                name="jobstatus",
            ),
            nullable=False,
        ),
        sa.Column(
            "priority",
            postgresql.ENUM("1", "2", "3", "4", name="jobpriority"),
            nullable=False,
        ),
        sa.Column(
            "parent_job_id",
            sa.Uuid(),
            nullable=True,
            comment="Parent job ID for workflow hierarchies",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
        ),
        sa.Column(
            "queued_at",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="When job entered queue",
        ),
        sa.Column(
            "started_at",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="When execution started",
        ),
        sa.Column(
            "completed_at",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="When job completed/failed",
        ),
        sa.Column(
            "paused_at",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="When job was paused",
        ),
        sa.Column(
            "timeout_seconds",
            sa.Integer(),
            nullable=True,
            comment="Auto-cancel if running longer than this",
        ),
        sa.Column(
            "scheduled_at",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="Delayed execution start time",
        ),
        sa.Column(
            "duration_ms",
            sa.Float(),
            nullable=True,
            comment="Execution duration in milliseconds",
        ),
        sa.Column(
            "cost_usd",
            sa.Numeric(precision=10, scale=6),
            nullable=False,
            comment="Total cost in USD",
        ),
        sa.Column(
            "result_data",
            postgresql.JSONB(astext_type=Text()),
            nullable=True,
            comment="Job result data",
        ),
        sa.Column(
            "error_message", sa.Text(), nullable=True, comment="Error message if failed"
        ),
        sa.Column(
            "retry_count",
            sa.Integer(),
            nullable=False,
            comment="Current retry attempt number",
        ),
        sa.Column(
            "max_retries",
            sa.Integer(),
            nullable=False,
            comment="Maximum retry attempts",
        ),
        sa.Column(
            "cancel_reason", sa.Text(), nullable=True, comment="Reason for cancellation"
        ),
        sa.ForeignKeyConstraint(
            ["parent_job_id"],
            ["jobs.id"],
            name=op.f("fk_jobs_parent_job_id_jobs"),
            ondelete="SET NULL",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_jobs")),
    )
    with op.batch_alter_table("jobs", schema=None) as batch_op:
        batch_op.create_index(
            "ix_jobs_completed_cleanup",
            ["completed_at"],
            unique=False,
            postgresql_where=sa.text("status IN ('completed', 'failed', 'cancelled')"),
        )
        batch_op.create_index(
            batch_op.f("ix_jobs_parent_job_id"), ["parent_job_id"], unique=False
        )
        batch_op.create_index(
            "ix_jobs_priority_queued",
            [sa.literal_column("priority DESC"), sa.literal_column("created_at ASC")],
            unique=False,
            postgresql_where=sa.text("status = 'queued'"),
        )
        batch_op.create_index(
            "ix_jobs_running_timeout",
            ["started_at"],
            unique=False,
            postgresql_where=sa.text(
                "status = 'running' AND timeout_seconds IS NOT NULL"
            ),
        )
        batch_op.create_index(batch_op.f("ix_jobs_status"), ["status"], unique=False)
        batch_op.create_index(
            batch_op.f("ix_jobs_task_name"), ["task_name"], unique=False
        )
        batch_op.create_index(
            "ix_jobs_task_name_status", ["task_name", "status"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_jobs_tenant_id"), ["tenant_id"], unique=False
        )
        batch_op.create_index(
            "ix_jobs_tenant_status", ["tenant_id", "status"], unique=False
        )

    op.create_table(
        "reminders",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("title", sa.String(length=200), nullable=False),
        sa.Column("description", sa.Text(), nullable=True),
        sa.Column("remind_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("is_completed", sa.Boolean(), nullable=False),
        sa.Column("notification_sent", sa.Boolean(), nullable=False),
        sa.Column(
            "recurrence_rule",
            sa.String(length=255),
            nullable=True,
            comment="iCalendar RRULE string for recurring reminders",
        ),
        sa.Column(
            "recurrence_end_at",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="When the recurrence series ends",
        ),
        sa.Column(
            "parent_id",
            sa.Uuid(),
            nullable=True,
            comment="Parent reminder ID for occurrences broken out from a series",
        ),
        sa.Column(
            "occurrence_date",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="Specific occurrence date for broken-out instances",
        ),
        sa.Column(
            "search_vector",
            TSVECTOR(),
            nullable=True,
            comment="Full-text search vector for title and description",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.Column(
            "tenant_id",
            sa.String(length=255),
            nullable=True,
            comment="Tenant ID for multi-tenant isolation",
        ),
        sa.ForeignKeyConstraint(
            ["parent_id"],
            ["reminders.id"],
            name=op.f("fk_reminders_parent_id_reminders"),
            ondelete="SET NULL",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_reminders")),
    )
    with op.batch_alter_table("reminders", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_reminders_parent_id"), ["parent_id"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_reminders_tenant_id"), ["tenant_id"], unique=False
        )

    op.create_table(
        "search_experiment_assignments",
        sa.Column("id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("experiment_id", sa.Integer(), nullable=False),
        sa.Column("experiment_name", sa.String(length=100), nullable=False),
        sa.Column("user_id", sa.String(length=255), nullable=False),
        sa.Column("variant", sa.String(length=100), nullable=False),
        sa.Column("assigned_at", sa.DateTime(), nullable=False),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_search_experiment_assignments")),
    )
    with op.batch_alter_table("search_experiment_assignments", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_search_experiment_assignments_experiment_id"),
            ["experiment_id"],
            unique=False,
        )
        batch_op.create_index(
            batch_op.f("ix_search_experiment_assignments_experiment_name"),
            ["experiment_name"],
            unique=False,
        )
        batch_op.create_index(
            batch_op.f("ix_search_experiment_assignments_user_id"),
            ["user_id"],
            unique=False,
        )

    op.create_table(
        "search_experiment_events",
        sa.Column("id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("experiment_id", sa.Integer(), nullable=False),
        sa.Column("experiment_name", sa.String(length=100), nullable=False),
        sa.Column("user_id", sa.String(length=255), nullable=False),
        sa.Column("variant", sa.String(length=100), nullable=False),
        sa.Column("event_type", sa.String(length=100), nullable=False),
        sa.Column("event_value", sa.Float(), nullable=True),
        sa.Column("event_data", postgresql.JSONB(astext_type=Text()), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_search_experiment_events")),
    )
    with op.batch_alter_table("search_experiment_events", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_search_experiment_events_event_type"),
            ["event_type"],
            unique=False,
        )
        batch_op.create_index(
            batch_op.f("ix_search_experiment_events_experiment_id"),
            ["experiment_id"],
            unique=False,
        )
        batch_op.create_index(
            batch_op.f("ix_search_experiment_events_experiment_name"),
            ["experiment_name"],
            unique=False,
        )
        batch_op.create_index(
            batch_op.f("ix_search_experiment_events_user_id"), ["user_id"], unique=False
        )

    op.create_table(
        "search_experiments",
        sa.Column("id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("name", sa.String(length=100), nullable=False),
        sa.Column("description", sa.String(length=500), nullable=True),
        sa.Column("status", sa.String(length=20), nullable=False),
        sa.Column("variants", postgresql.JSONB(astext_type=Text()), nullable=False),
        sa.Column("traffic_percentage", sa.Float(), nullable=False),
        sa.Column("start_date", sa.DateTime(), nullable=True),
        sa.Column("end_date", sa.DateTime(), nullable=True),
        sa.Column("owner", sa.String(length=100), nullable=True),
        sa.Column("hypothesis", sa.String(length=1000), nullable=True),
        sa.Column("primary_metric", sa.String(length=100), nullable=True),
        sa.Column(
            "experiment_data", postgresql.JSONB(astext_type=Text()), nullable=True
        ),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_search_experiments")),
    )
    with op.batch_alter_table("search_experiments", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_search_experiments_name"), ["name"], unique=True
        )
        batch_op.create_index(
            batch_op.f("ix_search_experiments_status"), ["status"], unique=False
        )

    op.create_table(
        "search_queries",
        sa.Column("id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("query_text", sa.String(length=500), nullable=False),
        sa.Column("query_hash", sa.String(length=64), nullable=False),
        sa.Column("normalized_query", sa.String(length=500), nullable=True),
        sa.Column("entity_types", postgresql.JSONB(astext_type=Text()), nullable=True),
        sa.Column("results_count", sa.Integer(), nullable=False),
        sa.Column("took_ms", sa.Integer(), nullable=False),
        sa.Column("user_id", sa.String(length=255), nullable=True),
        sa.Column("session_id", sa.String(length=255), nullable=True),
        sa.Column("clicked_result", sa.Boolean(), nullable=False),
        sa.Column("clicked_position", sa.Integer(), nullable=True),
        sa.Column("clicked_entity_id", sa.String(length=255), nullable=True),
        sa.Column("search_syntax", sa.String(length=50), nullable=True),
        sa.Column("metadata", postgresql.JSONB(astext_type=Text()), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_search_queries")),
    )
    with op.batch_alter_table("search_queries", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_search_queries_query_hash"), ["query_hash"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_search_queries_query_text"), ["query_text"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_search_queries_user_id"), ["user_id"], unique=False
        )

    op.create_table(
        "search_query_profiles",
        sa.Column("id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("query_type", sa.String(length=100), nullable=False),
        sa.Column("query_text", sa.String(length=1000), nullable=True),
        sa.Column("execution_time_ms", sa.Float(), nullable=False),
        sa.Column("is_slow", sa.Boolean(), nullable=False),
        sa.Column("entity_types", postgresql.JSONB(astext_type=Text()), nullable=True),
        sa.Column("result_count", sa.Integer(), nullable=True),
        sa.Column("profile_data", postgresql.JSONB(astext_type=Text()), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_search_query_profiles")),
    )
    with op.batch_alter_table("search_query_profiles", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_search_query_profiles_execution_time_ms"),
            ["execution_time_ms"],
            unique=False,
        )
        batch_op.create_index(
            batch_op.f("ix_search_query_profiles_is_slow"), ["is_slow"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_search_query_profiles_query_type"),
            ["query_type"],
            unique=False,
        )

    op.create_table(
        "search_suggestion_logs",
        sa.Column("id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("prefix", sa.String(length=100), nullable=False),
        sa.Column("suggested_text", sa.String(length=500), nullable=False),
        sa.Column("was_selected", sa.Boolean(), nullable=False),
        sa.Column("user_id", sa.String(length=255), nullable=True),
        sa.Column("session_id", sa.String(length=255), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_search_suggestion_logs")),
    )
    with op.batch_alter_table("search_suggestion_logs", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_search_suggestion_logs_prefix"), ["prefix"], unique=False
        )

    op.create_table(
        "tags",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column(
            "name",
            sa.String(length=50),
            nullable=False,
            comment="Unique tag name (e.g., 'work', 'urgent')",
        ),
        sa.Column(
            "color",
            sa.String(length=7),
            nullable=True,
            comment="Hex color code (e.g., '#FF5733')",
        ),
        sa.Column(
            "description",
            sa.String(length=200),
            nullable=True,
            comment="Optional description of the tag's purpose",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_tags")),
        sa.UniqueConstraint("name", name="uq_tags_name"),
    )
    with op.batch_alter_table("tags", schema=None) as batch_op:
        batch_op.create_index(batch_op.f("ix_tags_name"), ["name"], unique=False)

    op.create_table(
        "task_executions",
        sa.Column(
            "id",
            sa.Integer(),
            autoincrement=True,
            nullable=False,
            comment="Auto-incrementing primary key",
        ),
        sa.Column(
            "task_id",
            sa.String(length=255),
            nullable=False,
            comment="Unique task execution ID from Taskiq",
        ),
        sa.Column(
            "task_name",
            sa.String(length=255),
            nullable=False,
            comment="Task function name (e.g., 'backup_database')",
        ),
        sa.Column(
            "status",
            sa.String(length=50),
            nullable=False,
            comment="Status: pending, running, success, failure, cancelled",
        ),
        sa.Column(
            "worker_id",
            sa.String(length=255),
            nullable=True,
            comment="Worker that executed/is executing the task",
        ),
        sa.Column(
            "queue_name",
            sa.String(length=255),
            nullable=True,
            comment="Queue the task was sent to",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            nullable=False,
            comment="When the task was enqueued",
        ),
        sa.Column(
            "started_at",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="When execution started",
        ),
        sa.Column(
            "finished_at",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="When execution finished",
        ),
        sa.Column(
            "duration_ms",
            sa.Integer(),
            nullable=True,
            comment="Execution duration in milliseconds",
        ),
        sa.Column(
            "return_value",
            postgresql.JSONB(astext_type=Text()).with_variant(sa.JSON(), "sqlite"),
            nullable=True,
            comment="Task return value (JSON serialized)",
        ),
        sa.Column(
            "error_type",
            sa.String(length=255),
            nullable=True,
            comment="Exception class name if failed",
        ),
        sa.Column(
            "error_message", sa.Text(), nullable=True, comment="Error message if failed"
        ),
        sa.Column(
            "error_traceback",
            sa.Text(),
            nullable=True,
            comment="Full traceback if failed",
        ),
        sa.Column(
            "task_args",
            postgresql.JSONB(astext_type=Text()).with_variant(sa.JSON(), "sqlite"),
            nullable=True,
            comment="Task positional arguments (JSON)",
        ),
        sa.Column(
            "task_kwargs",
            postgresql.JSONB(astext_type=Text()).with_variant(sa.JSON(), "sqlite"),
            nullable=True,
            comment="Task keyword arguments (JSON)",
        ),
        sa.Column(
            "labels",
            postgresql.JSONB(astext_type=Text()).with_variant(sa.JSON(), "sqlite"),
            nullable=True,
            comment="Task labels/metadata for categorization",
        ),
        sa.Column(
            "retry_count",
            sa.Integer(),
            nullable=False,
            comment="Number of retry attempts",
        ),
        sa.Column(
            "max_retries",
            sa.Integer(),
            nullable=True,
            comment="Maximum retry attempts configured",
        ),
        sa.Column(
            "progress",
            postgresql.JSONB(astext_type=Text()).with_variant(sa.JSON(), "sqlite"),
            nullable=True,
            comment="Task progress data for long-running tasks",
        ),
        sa.Column(
            "serialized_result",
            sa.LargeBinary(),
            nullable=True,
            comment="Full serialized TaskiqResult for backend compatibility",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_task_executions")),
    )
    with op.batch_alter_table("task_executions", schema=None) as batch_op:
        batch_op.create_index(
            "ix_task_exec_created_desc",
            ["created_at"],
            unique=False,
            postgresql_using="btree",
        )
        batch_op.create_index(
            "ix_task_exec_name_status", ["task_name", "status"], unique=False
        )
        batch_op.create_index(
            "ix_task_exec_status_created", ["status", "created_at"], unique=False
        )
        batch_op.create_index(
            "ix_task_exec_worker_status", ["worker_id", "status"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_task_executions_created_at"), ["created_at"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_task_executions_error_type"), ["error_type"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_task_executions_status"), ["status"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_task_executions_task_id"), ["task_id"], unique=True
        )
        batch_op.create_index(
            batch_op.f("ix_task_executions_task_name"), ["task_name"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_task_executions_worker_id"), ["worker_id"], unique=False
        )

    op.create_table(
        "tenants",
        sa.Column("id", sa.String(length=255), nullable=False),
        sa.Column("name", sa.String(length=255), nullable=False),
        sa.Column("is_active", sa.Boolean(), nullable=False),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_tenants")),
    )
    op.create_table(
        "users",
        sa.Column("id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("email", sa.String(length=255), nullable=False),
        sa.Column("username", sa.String(length=50), nullable=False),
        sa.Column("hashed_password", sa.String(length=255), nullable=False),
        sa.Column("full_name", sa.String(length=255), nullable=True),
        sa.Column("is_active", sa.Boolean(), nullable=False),
        sa.Column("is_superuser", sa.Boolean(), nullable=False),
        sa.Column(
            "search_vector",
            TSVECTOR(),
            nullable=True,
            comment="Full-text search vector for email, username, and full_name",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_users")),
    )
    with op.batch_alter_table("users", schema=None) as batch_op:
        batch_op.create_index(batch_op.f("ix_users_email"), ["email"], unique=True)
        batch_op.create_index(
            "ix_users_email_username", ["email", "username"], unique=False
        )
        batch_op.create_index("ix_users_is_active", ["is_active"], unique=False)
        batch_op.create_index(
            batch_op.f("ix_users_username"), ["username"], unique=True
        )

    op.create_table(
        "webhooks",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column(
            "name",
            sa.String(length=200),
            nullable=False,
            comment="Human-readable webhook name",
        ),
        sa.Column(
            "description", sa.Text(), nullable=True, comment="Webhook description"
        ),
        sa.Column(
            "url",
            sa.String(length=2048),
            nullable=False,
            comment="Target URL for webhook delivery",
        ),
        sa.Column(
            "secret",
            sa.String(length=255),
            nullable=False,
            comment="HMAC secret for signing payloads",
        ),
        sa.Column(
            "event_types",
            StringArray(),
            nullable=False,
            comment="List of event types this webhook subscribes to",
        ),
        sa.Column(
            "is_active",
            sa.Boolean(),
            nullable=False,
            comment="Whether webhook is active",
        ),
        sa.Column(
            "max_retries",
            sa.Integer(),
            nullable=False,
            comment="Maximum delivery retry attempts",
        ),
        sa.Column(
            "timeout_seconds",
            sa.Integer(),
            nullable=False,
            comment="HTTP request timeout in seconds",
        ),
        sa.Column(
            "custom_headers",
            postgresql.JSONB(astext_type=Text()).with_variant(sa.JSON(), "sqlite"),
            nullable=True,
            comment="Additional HTTP headers to include in requests",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.Column(
            "tenant_id",
            sa.String(length=255),
            nullable=True,
            comment="Tenant ID for multi-tenant isolation",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_webhooks")),
    )
    with op.batch_alter_table("webhooks", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_webhooks_tenant_id"), ["tenant_id"], unique=False
        )

    op.create_table(
        "ai_agent_runs",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("tenant_id", sa.String(length=255), nullable=False),
        sa.Column(
            "agent_type",
            sa.String(length=255),
            nullable=False,
            comment="Type/name of the agent (e.g., 'research_agent', 'code_review')",
        ),
        sa.Column(
            "agent_version",
            sa.String(length=50),
            nullable=False,
            comment="Version of the agent definition",
        ),
        sa.Column(
            "run_name",
            sa.String(length=255),
            nullable=True,
            comment="Human-readable name for the run",
        ),
        sa.Column(
            "parent_run_id",
            sa.Uuid(),
            nullable=True,
            comment="Parent run ID if this is a sub-agent run",
        ),
        sa.Column(
            "status",
            postgresql.ENUM(
                "pending",
                "running",
                "paused",
                "waiting_input",
                "completed",
                "failed",
                "cancelled",
                "timeout",
                name="aiagentrunstatus",
            ),
            nullable=False,
        ),
        sa.Column(
            "status_message",
            sa.Text(),
            nullable=True,
            comment="Human-readable status message",
        ),
        sa.Column(
            "input_data", sa.JSON(), nullable=False, comment="Input data for the agent"
        ),
        sa.Column(
            "output_data",
            sa.JSON(),
            nullable=True,
            comment="Final output from the agent",
        ),
        sa.Column(
            "config",
            sa.JSON(),
            nullable=False,
            comment="Agent configuration (model, temperature, tools, etc.)",
        ),
        sa.Column(
            "state",
            sa.JSON(),
            nullable=False,
            comment="Current agent state (for pause/resume)",
        ),
        sa.Column(
            "context",
            sa.JSON(),
            nullable=False,
            comment="Shared context data between steps",
        ),
        sa.Column(
            "current_step", sa.Integer(), nullable=False, comment="Current step number"
        ),
        sa.Column(
            "total_steps",
            sa.Integer(),
            nullable=True,
            comment="Total expected steps (if known)",
        ),
        sa.Column(
            "progress_percent",
            sa.Float(),
            nullable=False,
            comment="Progress percentage 0-100",
        ),
        sa.Column(
            "total_cost_usd", sa.Float(), nullable=False, comment="Total cost in USD"
        ),
        sa.Column(
            "total_input_tokens",
            sa.Integer(),
            nullable=False,
            comment="Total input tokens consumed",
        ),
        sa.Column(
            "total_output_tokens",
            sa.Integer(),
            nullable=False,
            comment="Total output tokens generated",
        ),
        sa.Column(
            "retry_count",
            sa.Integer(),
            nullable=False,
            comment="Number of retry attempts",
        ),
        sa.Column(
            "max_retries",
            sa.Integer(),
            nullable=False,
            comment="Maximum retry attempts",
        ),
        sa.Column(
            "last_retry_at",
            sa.DateTime(),
            nullable=True,
            comment="Timestamp of last retry attempt",
        ),
        sa.Column(
            "error_message", sa.Text(), nullable=True, comment="Error message if failed"
        ),
        sa.Column(
            "error_code",
            sa.String(length=100),
            nullable=True,
            comment="Error code for programmatic handling",
        ),
        sa.Column(
            "error_details",
            sa.JSON(),
            nullable=True,
            comment="Detailed error information (stack trace, etc.)",
        ),
        sa.Column(
            "started_at", sa.DateTime(), nullable=True, comment="When execution started"
        ),
        sa.Column(
            "completed_at",
            sa.DateTime(),
            nullable=True,
            comment="When execution completed",
        ),
        sa.Column(
            "paused_at",
            sa.DateTime(),
            nullable=True,
            comment="When execution was paused",
        ),
        sa.Column(
            "timeout_seconds", sa.Integer(), nullable=True, comment="Timeout in seconds"
        ),
        sa.Column(
            "tags",
            sa.JSON(),
            nullable=False,
            comment="Tags for filtering/categorization",
        ),
        sa.Column(
            "metadata_json", sa.JSON(), nullable=False, comment="Additional metadata"
        ),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.Column("updated_at", sa.DateTime(), nullable=False),
        sa.Column("created_by_id", sa.Integer(), nullable=True),
        sa.ForeignKeyConstraint(
            ["created_by_id"],
            ["users.id"],
            name=op.f("fk_ai_agent_runs_created_by_id_users"),
            ondelete="SET NULL",
        ),
        sa.ForeignKeyConstraint(
            ["parent_run_id"],
            ["ai_agent_runs.id"],
            name=op.f("fk_ai_agent_runs_parent_run_id_ai_agent_runs"),
            ondelete="SET NULL",
        ),
        sa.ForeignKeyConstraint(
            ["tenant_id"],
            ["tenants.id"],
            name=op.f("fk_ai_agent_runs_tenant_id_tenants"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_ai_agent_runs")),
    )
    with op.batch_alter_table("ai_agent_runs", schema=None) as batch_op:
        batch_op.create_index(
            "ix_ai_agent_runs_created_at", ["created_at"], unique=False
        )
        batch_op.create_index(
            "ix_ai_agent_runs_parent", ["parent_run_id"], unique=False
        )
        batch_op.create_index(
            "ix_ai_agent_runs_status_created", ["status", "created_at"], unique=False
        )
        batch_op.create_index(
            "ix_ai_agent_runs_tenant_agent", ["tenant_id", "agent_type"], unique=False
        )
        batch_op.create_index(
            "ix_ai_agent_runs_tenant_status", ["tenant_id", "status"], unique=False
        )

    op.create_table(
        "ai_jobs",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("tenant_id", sa.String(length=255), nullable=False),
        sa.Column(
            "job_type",
            postgresql.ENUM(
                "TRANSCRIPTION",
                "PII_REDACTION",
                "SUMMARY",
                "SENTIMENT",
                "COACHING",
                "FULL_ANALYSIS",
                name="aijobtype",
            ),
            nullable=False,
        ),
        sa.Column(
            "status",
            postgresql.ENUM(
                "PENDING",
                "PROCESSING",
                "COMPLETED",
                "FAILED",
                "CANCELLED",
                name="aijobstatus",
            ),
            nullable=False,
        ),
        sa.Column(
            "input_data",
            sa.JSON(),
            nullable=False,
            comment="Job input parameters and data references",
        ),
        sa.Column(
            "result_data",
            sa.JSON(),
            nullable=True,
            comment="Processing results (transcripts, summaries, etc.)",
        ),
        sa.Column(
            "error_message", sa.Text(), nullable=True, comment="Error details if failed"
        ),
        sa.Column(
            "progress_percentage",
            sa.Integer(),
            nullable=False,
            comment="0-100 progress indicator",
        ),
        sa.Column(
            "current_step",
            sa.String(length=255),
            nullable=True,
            comment="Current processing step description",
        ),
        sa.Column(
            "started_at",
            sa.DateTime(),
            nullable=True,
            comment="When processing started",
        ),
        sa.Column(
            "completed_at",
            sa.DateTime(),
            nullable=True,
            comment="When processing completed",
        ),
        sa.Column(
            "duration_seconds",
            sa.Float(),
            nullable=True,
            comment="Total processing duration",
        ),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.Column("updated_at", sa.DateTime(), nullable=False),
        sa.Column("created_by_id", sa.Integer(), nullable=True),
        sa.ForeignKeyConstraint(
            ["created_by_id"],
            ["users.id"],
            name=op.f("fk_ai_jobs_created_by_id_users"),
            ondelete="SET NULL",
        ),
        sa.ForeignKeyConstraint(
            ["tenant_id"],
            ["tenants.id"],
            name=op.f("fk_ai_jobs_tenant_id_tenants"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_ai_jobs")),
    )
    with op.batch_alter_table("ai_jobs", schema=None) as batch_op:
        batch_op.create_index("ix_ai_jobs_created_at", ["created_at"], unique=False)
        batch_op.create_index(
            "ix_ai_jobs_status_created", ["status", "created_at"], unique=False
        )
        batch_op.create_index(
            "ix_ai_jobs_tenant_status", ["tenant_id", "status"], unique=False
        )
        batch_op.create_index(
            "ix_ai_jobs_tenant_type", ["tenant_id", "job_type"], unique=False
        )

    op.create_table(
        "ai_workflow_definitions",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("tenant_id", sa.String(length=255), nullable=False),
        sa.Column("name", sa.String(length=255), nullable=False),
        sa.Column("slug", sa.String(length=255), nullable=False),
        sa.Column("description", sa.Text(), nullable=True),
        sa.Column("version", sa.String(length=50), nullable=False),
        sa.Column("nodes", sa.JSON(), nullable=False),
        sa.Column("edges", sa.JSON(), nullable=False),
        sa.Column("entry_point", sa.String(length=255), nullable=False),
        sa.Column("end_nodes", sa.JSON(), nullable=False),
        sa.Column("default_config", sa.JSON(), nullable=False),
        sa.Column("timeout_seconds", sa.Integer(), nullable=True),
        sa.Column("max_retries", sa.Integer(), nullable=False),
        sa.Column("tags", sa.JSON(), nullable=False),
        sa.Column("is_active", sa.Boolean(), nullable=False),
        sa.Column("is_public", sa.Boolean(), nullable=False),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.Column("updated_at", sa.DateTime(), nullable=False),
        sa.Column("created_by_id", sa.Integer(), nullable=True),
        sa.ForeignKeyConstraint(
            ["created_by_id"],
            ["users.id"],
            name=op.f("fk_ai_workflow_definitions_created_by_id_users"),
            ondelete="SET NULL",
        ),
        sa.ForeignKeyConstraint(
            ["tenant_id"],
            ["tenants.id"],
            name=op.f("fk_ai_workflow_definitions_tenant_id_tenants"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_ai_workflow_definitions")),
    )
    with op.batch_alter_table("ai_workflow_definitions", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_ai_workflow_definitions_slug"), ["slug"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_ai_workflow_definitions_tenant_id"),
            ["tenant_id"],
            unique=False,
        )
        batch_op.create_index(
            "ix_workflow_definitions_tenant_active",
            ["tenant_id", "is_active"],
            unique=False,
        )
        batch_op.create_index(
            "ix_workflow_definitions_tenant_slug", ["tenant_id", "slug"], unique=True
        )

    op.create_table(
        "email_audit_logs",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column(
            "tenant_id",
            sa.String(length=255),
            nullable=True,
            comment="Tenant ID (nullable for system emails)",
        ),
        sa.Column(
            "recipient_hash",
            sa.String(length=64),
            nullable=False,
            comment="SHA256 hash of recipient email",
        ),
        sa.Column(
            "template_name",
            sa.String(length=255),
            nullable=True,
            comment="Template used",
        ),
        sa.Column(
            "subject_hash",
            sa.String(length=64),
            nullable=True,
            comment="SHA256 hash of subject (for duplicate detection)",
        ),
        sa.Column(
            "status",
            sa.String(length=50),
            nullable=False,
            comment="Status: queued, sent, failed, bounced",
        ),
        sa.Column(
            "provider", sa.String(length=50), nullable=True, comment="Provider used"
        ),
        sa.Column(
            "message_id",
            sa.String(length=255),
            nullable=True,
            comment="Provider message ID",
        ),
        sa.Column(
            "error_category",
            sa.String(length=50),
            nullable=True,
            comment="Error category if failed",
        ),
        sa.Column(
            "trace_id",
            sa.String(length=64),
            nullable=True,
            comment="OpenTelemetry trace ID",
        ),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(
            ["tenant_id"],
            ["tenants.id"],
            name=op.f("fk_email_audit_logs_tenant_id_tenants"),
            ondelete="SET NULL",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_email_audit_logs")),
    )
    with op.batch_alter_table("email_audit_logs", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_email_audit_logs_created_at"), ["created_at"], unique=False
        )
        batch_op.create_index(
            "ix_email_audit_logs_recipient",
            ["recipient_hash", "created_at"],
            unique=False,
        )
        batch_op.create_index(
            batch_op.f("ix_email_audit_logs_recipient_hash"),
            ["recipient_hash"],
            unique=False,
        )
        batch_op.create_index(
            "ix_email_audit_logs_status", ["status", "created_at"], unique=False
        )
        batch_op.create_index(
            "ix_email_audit_logs_tenant_created",
            ["tenant_id", "created_at"],
            unique=False,
        )
        batch_op.create_index(
            batch_op.f("ix_email_audit_logs_tenant_id"), ["tenant_id"], unique=False
        )

    op.create_table(
        "email_configs",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("tenant_id", sa.String(length=255), nullable=False),
        sa.Column(
            "provider_type",
            postgresql.ENUM(
                "smtp",
                "aws_ses",
                "sendgrid",
                "mailgun",
                "console",
                "file",
                name="emailprovidertype",
            ),
            nullable=False,
            comment="Email provider type (smtp, aws_ses, sendgrid, mailgun, console, file)",
        ),
        sa.Column(
            "is_active",
            sa.Boolean(),
            nullable=False,
            comment="Whether this config is active",
        ),
        sa.Column(
            "smtp_host",
            sa.String(length=255),
            nullable=True,
            comment="SMTP server hostname",
        ),
        sa.Column(
            "smtp_port",
            sa.Integer(),
            nullable=True,
            comment="SMTP server port (587 for TLS, 465 for SSL)",
        ),
        sa.Column(
            "smtp_username",
            sa.String(length=255),
            nullable=True,
            comment="SMTP authentication username",
        ),
        sa.Column(
            "smtp_password",
            EncryptedString(max_length=500),
            nullable=True,
            comment="SMTP authentication password (encrypted)",
        ),
        sa.Column(
            "smtp_use_tls",
            sa.Boolean(),
            nullable=True,
            comment="Use STARTTLS (port 587)",
        ),
        sa.Column(
            "smtp_use_ssl",
            sa.Boolean(),
            nullable=True,
            comment="Use implicit SSL (port 465)",
        ),
        sa.Column(
            "aws_region",
            sa.String(length=50),
            nullable=True,
            comment="AWS region for SES (e.g., us-east-1)",
        ),
        sa.Column(
            "aws_access_key",
            EncryptedString(max_length=500),
            nullable=True,
            comment="AWS access key ID (encrypted)",
        ),
        sa.Column(
            "aws_secret_key",
            EncryptedString(max_length=500),
            nullable=True,
            comment="AWS secret access key (encrypted)",
        ),
        sa.Column(
            "aws_configuration_set",
            sa.String(length=255),
            nullable=True,
            comment="AWS SES configuration set name for tracking",
        ),
        sa.Column(
            "api_key",
            EncryptedString(max_length=500),
            nullable=True,
            comment="API key for SendGrid/Mailgun (encrypted)",
        ),
        sa.Column(
            "api_endpoint",
            sa.String(length=255),
            nullable=True,
            comment="Custom API endpoint (for Mailgun EU, etc.)",
        ),
        sa.Column(
            "from_email",
            sa.String(length=255),
            nullable=True,
            comment="Default sender email address",
        ),
        sa.Column(
            "from_name",
            sa.String(length=255),
            nullable=True,
            comment="Default sender display name",
        ),
        sa.Column(
            "reply_to",
            sa.String(length=255),
            nullable=True,
            comment="Default reply-to address",
        ),
        sa.Column(
            "rate_limit_per_minute",
            sa.Integer(),
            nullable=True,
            comment="Max emails per minute (None = use system default)",
        ),
        sa.Column(
            "rate_limit_per_hour",
            sa.Integer(),
            nullable=True,
            comment="Max emails per hour (None = use system default)",
        ),
        sa.Column(
            "daily_quota",
            sa.Integer(),
            nullable=True,
            comment="Max emails per day (None = unlimited)",
        ),
        sa.Column(
            "cost_per_email_usd",
            sa.Float(),
            nullable=True,
            comment="Cost per email in USD for billing calculation",
        ),
        sa.Column(
            "monthly_budget_usd",
            sa.Float(),
            nullable=True,
            comment="Monthly email spending limit in USD",
        ),
        sa.Column(
            "config_json",
            sa.JSON(),
            nullable=True,
            comment="Additional provider-specific configuration",
        ),
        sa.Column(
            "encryption_version",
            sa.Integer(),
            nullable=False,
            comment="Encryption key version for rotation",
        ),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.Column("updated_at", sa.DateTime(), nullable=False),
        sa.Column("created_by_id", sa.Integer(), nullable=True),
        sa.ForeignKeyConstraint(
            ["created_by_id"],
            ["users.id"],
            name=op.f("fk_email_configs_created_by_id_users"),
            ondelete="SET NULL",
        ),
        sa.ForeignKeyConstraint(
            ["tenant_id"],
            ["tenants.id"],
            name=op.f("fk_email_configs_tenant_id_tenants"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_email_configs")),
    )
    with op.batch_alter_table("email_configs", schema=None) as batch_op:
        batch_op.create_index(
            "ix_email_configs_provider", ["provider_type"], unique=False
        )
        batch_op.create_index(
            "ix_email_configs_tenant_active", ["tenant_id", "is_active"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_email_configs_tenant_id"), ["tenant_id"], unique=True
        )

    op.create_table(
        "email_usage_logs",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("tenant_id", sa.String(length=255), nullable=False),
        sa.Column(
            "provider",
            sa.String(length=50),
            nullable=False,
            comment="Provider used (smtp, sendgrid, etc.)",
        ),
        sa.Column(
            "recipients_count",
            sa.Integer(),
            nullable=False,
            comment="Number of recipients",
        ),
        sa.Column(
            "cost_usd", sa.Float(), nullable=True, comment="Calculated cost in USD"
        ),
        sa.Column(
            "success",
            sa.Boolean(),
            nullable=False,
            comment="Whether delivery succeeded",
        ),
        sa.Column(
            "duration_ms",
            sa.Integer(),
            nullable=True,
            comment="Delivery duration in milliseconds",
        ),
        sa.Column(
            "message_id",
            sa.String(length=255),
            nullable=True,
            comment="Provider message ID for tracking",
        ),
        sa.Column(
            "error_category",
            sa.String(length=50),
            nullable=True,
            comment="Error category if failed (auth, network, recipient, quota)",
        ),
        sa.Column(
            "error_message", sa.Text(), nullable=True, comment="Error details if failed"
        ),
        sa.Column(
            "trace_id",
            sa.String(length=64),
            nullable=True,
            comment="OpenTelemetry trace ID for correlation",
        ),
        sa.Column(
            "template_name",
            sa.String(length=255),
            nullable=True,
            comment="Template used (if any)",
        ),
        sa.Column(
            "metadata_json",
            sa.JSON(),
            nullable=True,
            comment="Additional operation metadata",
        ),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(
            ["tenant_id"],
            ["tenants.id"],
            name=op.f("fk_email_usage_logs_tenant_id_tenants"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_email_usage_logs")),
    )
    with op.batch_alter_table("email_usage_logs", schema=None) as batch_op:
        batch_op.create_index(
            "ix_email_usage_logs_created_at", ["created_at"], unique=False
        )
        batch_op.create_index(
            "ix_email_usage_logs_tenant_created",
            ["tenant_id", "created_at"],
            unique=False,
        )
        batch_op.create_index(
            batch_op.f("ix_email_usage_logs_tenant_id"), ["tenant_id"], unique=False
        )
        batch_op.create_index(
            "ix_email_usage_logs_tenant_provider",
            ["tenant_id", "provider"],
            unique=False,
        )

    op.create_table(
        "file_thumbnails",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("file_id", sa.Uuid(), nullable=False),
        sa.Column("storage_key", sa.String(length=500), nullable=False),
        sa.Column("width", sa.Integer(), nullable=False),
        sa.Column("height", sa.Integer(), nullable=False),
        sa.Column("size_bytes", sa.Integer(), nullable=False),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.ForeignKeyConstraint(
            ["file_id"],
            ["files.id"],
            name=op.f("fk_file_thumbnails_file_id_files"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_file_thumbnails")),
        sa.UniqueConstraint("storage_key", name=op.f("uq_file_thumbnails_storage_key")),
    )
    with op.batch_alter_table("file_thumbnails", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_file_thumbnails_file_id"), ["file_id"], unique=False
        )

    op.create_table(
        "job_audit_logs",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("job_id", sa.Uuid(), nullable=False),
        sa.Column(
            "from_status",
            sa.Enum(
                "PENDING",
                "QUEUED",
                "RUNNING",
                "COMPLETED",
                "FAILED",
                "CANCELLED",
                "RETRYING",
                "PAUSED",
                name="jobstatus",
            ),
            nullable=True,
            comment="Previous status (None for creation)",
        ),
        sa.Column(
            "to_status",
            sa.Enum(
                "PENDING",
                "QUEUED",
                "RUNNING",
                "COMPLETED",
                "FAILED",
                "CANCELLED",
                "RETRYING",
                "PAUSED",
                name="jobstatus",
            ),
            nullable=False,
            comment="New status",
        ),
        sa.Column(
            "triggered_by",
            sa.String(length=100),
            nullable=False,
            comment="What triggered the transition: user, system, timeout, dependency",
        ),
        sa.Column(
            "actor_id",
            sa.String(length=255),
            nullable=True,
            comment="User ID if user-triggered",
        ),
        sa.Column(
            "reason",
            sa.Text(),
            nullable=True,
            comment="Human-readable reason for transition",
        ),
        sa.Column(
            "extra_data",
            postgresql.JSONB(astext_type=Text()),
            nullable=True,
            comment="Additional context data",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
        ),
        sa.ForeignKeyConstraint(
            ["job_id"],
            ["jobs.id"],
            name=op.f("fk_job_audit_logs_job_id_jobs"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_job_audit_logs")),
    )
    with op.batch_alter_table("job_audit_logs", schema=None) as batch_op:
        batch_op.create_index(
            "ix_job_audit_job_created", ["job_id", "created_at"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_job_audit_logs_created_at"), ["created_at"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_job_audit_logs_job_id"), ["job_id"], unique=False
        )

    op.create_table(
        "job_dependencies",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("job_id", sa.Uuid(), nullable=False),
        sa.Column("depends_on_job_id", sa.Uuid(), nullable=False),
        sa.Column(
            "required",
            sa.Boolean(),
            nullable=False,
            comment="True = hard dependency (blocks), False = soft (ordering hint)",
        ),
        sa.Column(
            "satisfied",
            sa.Boolean(),
            nullable=False,
            comment="Whether dependency is satisfied",
        ),
        sa.Column(
            "satisfied_at",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="When dependency was satisfied",
        ),
        sa.ForeignKeyConstraint(
            ["depends_on_job_id"],
            ["jobs.id"],
            name=op.f("fk_job_dependencies_depends_on_job_id_jobs"),
            ondelete="CASCADE",
        ),
        sa.ForeignKeyConstraint(
            ["job_id"],
            ["jobs.id"],
            name=op.f("fk_job_dependencies_job_id_jobs"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_job_dependencies")),
        sa.UniqueConstraint("job_id", "depends_on_job_id", name="uq_job_dependency"),
    )
    with op.batch_alter_table("job_dependencies", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_job_dependencies_depends_on_job_id"),
            ["depends_on_job_id"],
            unique=False,
        )
        batch_op.create_index(
            batch_op.f("ix_job_dependencies_job_id"), ["job_id"], unique=False
        )
        batch_op.create_index(
            "ix_job_deps_unsatisfied",
            ["job_id"],
            unique=False,
            postgresql_where=sa.text("NOT satisfied"),
        )

    op.create_table(
        "job_labels",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("job_id", sa.Uuid(), nullable=False),
        sa.Column("key", sa.String(length=100), nullable=False, comment="Label key"),
        sa.Column(
            "value", sa.String(length=255), nullable=False, comment="Label value"
        ),
        sa.ForeignKeyConstraint(
            ["job_id"],
            ["jobs.id"],
            name=op.f("fk_job_labels_job_id_jobs"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_job_labels")),
        sa.UniqueConstraint("job_id", "key", name="uq_job_label_key"),
    )
    with op.batch_alter_table("job_labels", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_job_labels_job_id"), ["job_id"], unique=False
        )
        batch_op.create_index("ix_job_labels_key_value", ["key", "value"], unique=False)

    op.create_table(
        "job_progress",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("job_id", sa.Uuid(), nullable=False),
        sa.Column(
            "percentage",
            sa.Integer(),
            nullable=False,
            comment="0-100 percentage complete",
        ),
        sa.Column(
            "current_stage",
            sa.String(length=255),
            nullable=True,
            comment="Current stage name",
        ),
        sa.Column(
            "total_stages",
            sa.Integer(),
            nullable=False,
            comment="Total number of stages",
        ),
        sa.Column(
            "completed_stages",
            sa.Integer(),
            nullable=False,
            comment="Completed stages count",
        ),
        sa.Column(
            "current_item",
            sa.Integer(),
            nullable=False,
            comment="Current item being processed",
        ),
        sa.Column(
            "total_items",
            sa.Integer(),
            nullable=False,
            comment="Total items to process",
        ),
        sa.Column(
            "estimated_completion",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="Estimated completion time",
        ),
        sa.Column(
            "message",
            sa.Text(),
            nullable=True,
            comment="Custom progress message for display",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
        ),
        sa.ForeignKeyConstraint(
            ["job_id"],
            ["jobs.id"],
            name=op.f("fk_job_progress_job_id_jobs"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_job_progress")),
        sa.UniqueConstraint("job_id", name=op.f("uq_job_progress_job_id")),
    )
    op.create_table(
        "job_webhooks",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("job_id", sa.Uuid(), nullable=False),
        sa.Column(
            "url",
            sa.String(length=2048),
            nullable=False,
            comment="Webhook endpoint URL",
        ),
        sa.Column(
            "secret",
            sa.String(length=255),
            nullable=True,
            comment="HMAC secret for signature verification",
        ),
        sa.Column(
            "on_completed",
            sa.Boolean(),
            nullable=False,
            comment="Fire on successful completion",
        ),
        sa.Column("on_failed", sa.Boolean(), nullable=False, comment="Fire on failure"),
        sa.Column(
            "on_cancelled", sa.Boolean(), nullable=False, comment="Fire on cancellation"
        ),
        sa.Column(
            "on_progress",
            sa.Boolean(),
            nullable=False,
            comment="Fire on progress updates (debounced)",
        ),
        sa.Column(
            "last_attempt_at",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="Last delivery attempt",
        ),
        sa.Column(
            "last_success_at",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="Last successful delivery",
        ),
        sa.Column(
            "failure_count",
            sa.Integer(),
            nullable=False,
            comment="Consecutive failures",
        ),
        sa.ForeignKeyConstraint(
            ["job_id"],
            ["jobs.id"],
            name=op.f("fk_job_webhooks_job_id_jobs"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_job_webhooks")),
    )
    with op.batch_alter_table("job_webhooks", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_job_webhooks_job_id"), ["job_id"], unique=False
        )

    op.create_table(
        "posts",
        sa.Column("id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("title", sa.String(length=255), nullable=False),
        sa.Column("content", sa.Text(), nullable=False),
        sa.Column("slug", sa.String(length=255), nullable=False),
        sa.Column("is_published", sa.Boolean(), nullable=False),
        sa.Column("author_id", sa.Integer(), nullable=False),
        sa.Column(
            "search_vector",
            TSVECTOR(),
            nullable=True,
            comment="Full-text search vector for title, content, and slug",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.ForeignKeyConstraint(
            ["author_id"],
            ["users.id"],
            name=op.f("fk_posts_author_id_users"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_posts")),
        sa.UniqueConstraint("slug", name=op.f("uq_posts_slug")),
    )
    with op.batch_alter_table("posts", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_posts_author_id"), ["author_id"], unique=False
        )
        batch_op.create_index(
            "ix_posts_author_id_is_published",
            ["author_id", "is_published"],
            unique=False,
        )
        batch_op.create_index("ix_posts_slug", ["slug"], unique=False)

    op.create_table(
        "reminder_tags",
        sa.Column("reminder_id", sa.Uuid(), nullable=False),
        sa.Column("tag_id", sa.Uuid(), nullable=False),
        sa.ForeignKeyConstraint(
            ["reminder_id"],
            ["reminders.id"],
            name=op.f("fk_reminder_tags_reminder_id_reminders"),
            ondelete="CASCADE",
        ),
        sa.ForeignKeyConstraint(
            ["tag_id"],
            ["tags.id"],
            name=op.f("fk_reminder_tags_tag_id_tags"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("reminder_id", "tag_id", name=op.f("pk_reminder_tags")),
    )
    op.create_table(
        "tenant_ai_configs",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("tenant_id", sa.String(length=255), nullable=False),
        sa.Column(
            "provider_type",
            postgresql.ENUM(
                "LLM",
                "TRANSCRIPTION",
                "EMBEDDING",
                "IMAGE",
                "PII_REDACTION",
                name="aiprovidertype",
            ),
            nullable=False,
        ),
        sa.Column(
            "provider_name",
            sa.String(length=100),
            nullable=False,
            comment="openai|anthropic|deepgram|etc",
        ),
        sa.Column(
            "model_name",
            sa.String(length=255),
            nullable=True,
            comment="Optional model override (e.g., gpt-4, claude-3)",
        ),
        sa.Column(
            "encrypted_api_key",
            sa.Text(),
            nullable=True,
            comment="Encrypted API key for provider",
        ),
        sa.Column(
            "config_json",
            sa.JSON(),
            nullable=True,
            comment="Additional provider-specific configuration",
        ),
        sa.Column("is_active", sa.Boolean(), nullable=False),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.Column("updated_at", sa.DateTime(), nullable=False),
        sa.Column("created_by_id", sa.Integer(), nullable=True),
        sa.ForeignKeyConstraint(
            ["created_by_id"],
            ["users.id"],
            name=op.f("fk_tenant_ai_configs_created_by_id_users"),
            ondelete="SET NULL",
        ),
        sa.ForeignKeyConstraint(
            ["tenant_id"],
            ["tenants.id"],
            name=op.f("fk_tenant_ai_configs_tenant_id_tenants"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_tenant_ai_configs")),
    )
    with op.batch_alter_table("tenant_ai_configs", schema=None) as batch_op:
        batch_op.create_index(
            "ix_tenant_ai_configs_tenant_active",
            ["tenant_id", "is_active"],
            unique=False,
        )
        batch_op.create_index(
            "ix_tenant_ai_configs_tenant_provider",
            ["tenant_id", "provider_type", "provider_name"],
            unique=False,
        )

    op.create_table(
        "tenant_ai_features",
        sa.Column("tenant_id", sa.String(length=255), nullable=False),
        sa.Column("transcription_enabled", sa.Boolean(), nullable=False),
        sa.Column("pii_redaction_enabled", sa.Boolean(), nullable=False),
        sa.Column("summary_enabled", sa.Boolean(), nullable=False),
        sa.Column("sentiment_enabled", sa.Boolean(), nullable=False),
        sa.Column("coaching_enabled", sa.Boolean(), nullable=False),
        sa.Column(
            "pii_entity_types",
            sa.JSON(),
            nullable=True,
            comment="List of PII entity types to detect/redact",
        ),
        sa.Column(
            "pii_confidence_threshold",
            sa.Float(),
            nullable=True,
            comment="Minimum confidence for PII detection (0.0-1.0)",
        ),
        sa.Column(
            "max_audio_duration_seconds",
            sa.Integer(),
            nullable=True,
            comment="Maximum audio duration for transcription (override global)",
        ),
        sa.Column(
            "max_concurrent_jobs",
            sa.Integer(),
            nullable=True,
            comment="Maximum concurrent AI jobs (override global)",
        ),
        sa.Column(
            "monthly_budget_usd",
            sa.Float(),
            nullable=True,
            comment="Monthly AI spending limit in USD",
        ),
        sa.Column(
            "enable_cost_alerts",
            sa.Boolean(),
            nullable=False,
            comment="Send alerts on high costs",
        ),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.Column("updated_at", sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(
            ["tenant_id"],
            ["tenants.id"],
            name=op.f("fk_tenant_ai_features_tenant_id_tenants"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("tenant_id", name=op.f("pk_tenant_ai_features")),
    )
    op.create_table(
        "webhook_deliveries",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column(
            "webhook_id",
            sa.Uuid(),
            nullable=False,
            comment="Reference to webhook configuration",
        ),
        sa.Column(
            "event_type",
            sa.String(length=100),
            nullable=False,
            comment="Type of event being delivered",
        ),
        sa.Column(
            "event_id",
            sa.String(length=255),
            nullable=False,
            comment="Unique identifier for the event",
        ),
        sa.Column(
            "payload",
            postgresql.JSONB(astext_type=Text()).with_variant(sa.JSON(), "sqlite"),
            nullable=False,
            comment="Event payload data",
        ),
        sa.Column(
            "status",
            postgresql.ENUM(
                "pending", "delivered", "failed", "retrying", name="deliverystatus"
            ),
            nullable=False,
            comment="Delivery status: pending, delivered, failed, retrying",
        ),
        sa.Column(
            "attempt_count",
            sa.Integer(),
            nullable=False,
            comment="Number of delivery attempts made",
        ),
        sa.Column(
            "max_attempts",
            sa.Integer(),
            nullable=False,
            comment="Maximum attempts allowed",
        ),
        sa.Column(
            "next_retry_at",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="Scheduled time for next retry attempt",
        ),
        sa.Column(
            "response_status_code",
            sa.Integer(),
            nullable=True,
            comment="HTTP response status code",
        ),
        sa.Column(
            "response_body",
            sa.Text(),
            nullable=True,
            comment="HTTP response body (truncated)",
        ),
        sa.Column(
            "response_time_ms",
            sa.Integer(),
            nullable=True,
            comment="Response time in milliseconds",
        ),
        sa.Column(
            "error_message",
            sa.Text(),
            nullable=True,
            comment="Error message if delivery failed",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of record creation",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("(CURRENT_TIMESTAMP)"),
            nullable=False,
            comment="Timestamp of last update",
        ),
        sa.ForeignKeyConstraint(
            ["webhook_id"],
            ["webhooks.id"],
            name=op.f("fk_webhook_deliveries_webhook_id_webhooks"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_webhook_deliveries")),
    )
    with op.batch_alter_table("webhook_deliveries", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_webhook_deliveries_event_id"), ["event_id"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_webhook_deliveries_event_type"), ["event_type"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_webhook_deliveries_next_retry_at"),
            ["next_retry_at"],
            unique=False,
        )
        batch_op.create_index(
            batch_op.f("ix_webhook_deliveries_status"), ["status"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_webhook_deliveries_webhook_id"), ["webhook_id"], unique=False
        )

    op.create_table(
        "ai_agent_checkpoints",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("run_id", sa.Uuid(), nullable=False),
        sa.Column(
            "checkpoint_name",
            sa.String(length=255),
            nullable=False,
            comment="Human-readable checkpoint name",
        ),
        sa.Column(
            "step_number",
            sa.Integer(),
            nullable=False,
            comment="Step number at checkpoint",
        ),
        sa.Column(
            "state_snapshot",
            sa.JSON(),
            nullable=False,
            comment="Agent state at checkpoint",
        ),
        sa.Column(
            "context_snapshot",
            sa.JSON(),
            nullable=False,
            comment="Context data at checkpoint",
        ),
        sa.Column(
            "messages_snapshot",
            sa.JSON(),
            nullable=False,
            comment="Conversation messages at checkpoint",
        ),
        sa.Column(
            "is_automatic",
            sa.Boolean(),
            nullable=False,
            comment="Whether checkpoint was auto-created",
        ),
        sa.Column(
            "trigger_reason",
            sa.String(length=255),
            nullable=True,
            comment="Reason for checkpoint creation",
        ),
        sa.Column(
            "is_valid",
            sa.Boolean(),
            nullable=False,
            comment="Whether checkpoint is valid for resume",
        ),
        sa.Column(
            "invalidated_reason",
            sa.String(length=255),
            nullable=True,
            comment="Reason if checkpoint was invalidated",
        ),
        sa.Column("metadata_json", sa.JSON(), nullable=False),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(
            ["run_id"],
            ["ai_agent_runs.id"],
            name=op.f("fk_ai_agent_checkpoints_run_id_ai_agent_runs"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_ai_agent_checkpoints")),
    )
    with op.batch_alter_table("ai_agent_checkpoints", schema=None) as batch_op:
        batch_op.create_index("ix_ai_agent_checkpoints_run", ["run_id"], unique=False)
        batch_op.create_index(
            "ix_ai_agent_checkpoints_run_step", ["run_id", "step_number"], unique=False
        )
        batch_op.create_index(
            "ix_ai_agent_checkpoints_valid", ["run_id", "is_valid"], unique=False
        )

    op.create_table(
        "ai_agent_steps",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("run_id", sa.Uuid(), nullable=False),
        sa.Column(
            "step_number",
            sa.Integer(),
            nullable=False,
            comment="Sequential step number within run",
        ),
        sa.Column(
            "step_type",
            postgresql.ENUM(
                "llm_call",
                "tool_call",
                "human_input",
                "checkpoint",
                "branch",
                "parallel",
                "subagent",
                name="aiagentsteptype",
            ),
            nullable=False,
        ),
        sa.Column(
            "step_name",
            sa.String(length=255),
            nullable=False,
            comment="Descriptive name for the step",
        ),
        sa.Column(
            "parent_step_id",
            sa.Uuid(),
            nullable=True,
            comment="Parent step ID for nested/parallel steps",
        ),
        sa.Column(
            "status",
            postgresql.ENUM(
                "pending",
                "running",
                "completed",
                "failed",
                "skipped",
                "retrying",
                name="aiagentstepstatus",
            ),
            nullable=False,
        ),
        sa.Column(
            "input_data", sa.JSON(), nullable=False, comment="Input data for the step"
        ),
        sa.Column(
            "output_data", sa.JSON(), nullable=True, comment="Output data from the step"
        ),
        sa.Column(
            "provider_name",
            sa.String(length=100),
            nullable=True,
            comment="Provider used (openai, anthropic, etc.)",
        ),
        sa.Column(
            "model_name",
            sa.String(length=255),
            nullable=True,
            comment="Model used for LLM calls",
        ),
        sa.Column("cost_usd", sa.Float(), nullable=False),
        sa.Column("input_tokens", sa.Integer(), nullable=False),
        sa.Column("output_tokens", sa.Integer(), nullable=False),
        sa.Column("started_at", sa.DateTime(), nullable=True),
        sa.Column("completed_at", sa.DateTime(), nullable=True),
        sa.Column(
            "duration_ms",
            sa.Float(),
            nullable=True,
            comment="Step duration in milliseconds",
        ),
        sa.Column("attempt_number", sa.Integer(), nullable=False),
        sa.Column("max_attempts", sa.Integer(), nullable=False),
        sa.Column("error_message", sa.Text(), nullable=True),
        sa.Column("error_code", sa.String(length=100), nullable=True),
        sa.Column("is_retryable", sa.Boolean(), nullable=False),
        sa.Column("metadata_json", sa.JSON(), nullable=False),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(
            ["parent_step_id"],
            ["ai_agent_steps.id"],
            name=op.f("fk_ai_agent_steps_parent_step_id_ai_agent_steps"),
            ondelete="SET NULL",
        ),
        sa.ForeignKeyConstraint(
            ["run_id"],
            ["ai_agent_runs.id"],
            name=op.f("fk_ai_agent_steps_run_id_ai_agent_runs"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_ai_agent_steps")),
    )
    with op.batch_alter_table("ai_agent_steps", schema=None) as batch_op:
        batch_op.create_index("ix_ai_agent_steps_run", ["run_id"], unique=False)
        batch_op.create_index(
            "ix_ai_agent_steps_run_step", ["run_id", "step_number"], unique=False
        )
        batch_op.create_index("ix_ai_agent_steps_status", ["status"], unique=False)

    op.create_table(
        "ai_usage_logs",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("tenant_id", sa.String(length=255), nullable=False),
        sa.Column("job_id", sa.Uuid(), nullable=True, comment="Associated AI job"),
        sa.Column(
            "provider_name",
            sa.String(length=100),
            nullable=False,
            comment="openai|anthropic|deepgram|etc",
        ),
        sa.Column(
            "model_name",
            sa.String(length=255),
            nullable=False,
            comment="Specific model used (gpt-4, nova-2, etc)",
        ),
        sa.Column(
            "operation_type",
            sa.String(length=100),
            nullable=False,
            comment="transcription|llm_generation|embedding|pii_detection",
        ),
        sa.Column(
            "input_tokens", sa.Integer(), nullable=True, comment="Input tokens (LLM)"
        ),
        sa.Column(
            "output_tokens", sa.Integer(), nullable=True, comment="Output tokens (LLM)"
        ),
        sa.Column(
            "audio_seconds",
            sa.Float(),
            nullable=True,
            comment="Audio duration (transcription)",
        ),
        sa.Column(
            "characters_processed",
            sa.Integer(),
            nullable=True,
            comment="Characters processed (PII, etc.)",
        ),
        sa.Column(
            "cost_usd", sa.Float(), nullable=False, comment="Calculated cost in USD"
        ),
        sa.Column(
            "cost_calculation_method",
            sa.String(length=50),
            nullable=True,
            comment="How cost was calculated (api_reported|estimated)",
        ),
        sa.Column(
            "duration_seconds", sa.Float(), nullable=False, comment="Operation duration"
        ),
        sa.Column(
            "success",
            sa.Boolean(),
            nullable=False,
            comment="Whether operation succeeded",
        ),
        sa.Column("error_message", sa.Text(), nullable=True, comment="Error if failed"),
        sa.Column(
            "metadata_json",
            sa.JSON(),
            nullable=True,
            comment="Additional operation-specific metadata",
        ),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(
            ["job_id"],
            ["ai_jobs.id"],
            name=op.f("fk_ai_usage_logs_job_id_ai_jobs"),
            ondelete="SET NULL",
        ),
        sa.ForeignKeyConstraint(
            ["tenant_id"],
            ["tenants.id"],
            name=op.f("fk_ai_usage_logs_tenant_id_tenants"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_ai_usage_logs")),
    )
    with op.batch_alter_table("ai_usage_logs", schema=None) as batch_op:
        batch_op.create_index(
            "ix_ai_usage_logs_created_at", ["created_at"], unique=False
        )
        batch_op.create_index("ix_ai_usage_logs_job", ["job_id"], unique=False)
        batch_op.create_index(
            "ix_ai_usage_logs_tenant_created", ["tenant_id", "created_at"], unique=False
        )
        batch_op.create_index(
            "ix_ai_usage_logs_tenant_provider",
            ["tenant_id", "provider_name"],
            unique=False,
        )

    op.create_table(
        "ai_workflow_executions",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("definition_id", sa.Uuid(), nullable=False),
        sa.Column("tenant_id", sa.String(length=255), nullable=False),
        sa.Column(
            "status",
            postgresql.ENUM(
                "pending",
                "running",
                "paused",
                "waiting_approval",
                "completed",
                "failed",
                "cancelled",
                "timeout",
                name="aiworkflowstatus",
            ),
            nullable=False,
        ),
        sa.Column("current_node", sa.String(length=255), nullable=True),
        sa.Column("executed_nodes", sa.JSON(), nullable=False),
        sa.Column("input_data", sa.JSON(), nullable=False),
        sa.Column("state_data", sa.JSON(), nullable=False),
        sa.Column("output_data", sa.JSON(), nullable=True),
        sa.Column("config", sa.JSON(), nullable=False),
        sa.Column("error", sa.Text(), nullable=True),
        sa.Column("error_code", sa.String(length=100), nullable=True),
        sa.Column("failed_node", sa.String(length=255), nullable=True),
        sa.Column("started_at", sa.DateTime(), nullable=True),
        sa.Column("completed_at", sa.DateTime(), nullable=True),
        sa.Column("paused_at", sa.DateTime(), nullable=True),
        sa.Column("total_cost_usd", sa.Float(), nullable=False),
        sa.Column("total_tokens", sa.Integer(), nullable=False),
        sa.Column("retry_count", sa.Integer(), nullable=False),
        sa.Column("parent_execution_id", sa.Uuid(), nullable=True),
        sa.Column("correlation_id", sa.String(length=255), nullable=True),
        sa.Column("tags", sa.JSON(), nullable=False),
        sa.Column("metadata", sa.JSON(), nullable=False),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.Column("updated_at", sa.DateTime(), nullable=False),
        sa.Column("created_by_id", sa.Integer(), nullable=True),
        sa.ForeignKeyConstraint(
            ["created_by_id"],
            ["users.id"],
            name=op.f("fk_ai_workflow_executions_created_by_id_users"),
            ondelete="SET NULL",
        ),
        sa.ForeignKeyConstraint(
            ["definition_id"],
            ["ai_workflow_definitions.id"],
            name=op.f(
                "fk_ai_workflow_executions_definition_id_ai_workflow_definitions"
            ),
            ondelete="CASCADE",
        ),
        sa.ForeignKeyConstraint(
            ["parent_execution_id"],
            ["ai_workflow_executions.id"],
            name=op.f(
                "fk_ai_workflow_executions_parent_execution_id_ai_workflow_executions"
            ),
            ondelete="SET NULL",
        ),
        sa.ForeignKeyConstraint(
            ["tenant_id"],
            ["tenants.id"],
            name=op.f("fk_ai_workflow_executions_tenant_id_tenants"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_ai_workflow_executions")),
    )
    with op.batch_alter_table("ai_workflow_executions", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_ai_workflow_executions_definition_id"),
            ["definition_id"],
            unique=False,
        )
        batch_op.create_index(
            batch_op.f("ix_ai_workflow_executions_status"), ["status"], unique=False
        )
        batch_op.create_index(
            batch_op.f("ix_ai_workflow_executions_tenant_id"),
            ["tenant_id"],
            unique=False,
        )
        batch_op.create_index(
            "ix_workflow_executions_created", ["tenant_id", "created_at"], unique=False
        )
        batch_op.create_index(
            "ix_workflow_executions_definition",
            ["definition_id", "status"],
            unique=False,
        )
        batch_op.create_index(
            "ix_workflow_executions_tenant_status",
            ["tenant_id", "status"],
            unique=False,
        )

    op.create_table(
        "ai_agent_messages",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("run_id", sa.Uuid(), nullable=False),
        sa.Column(
            "step_id",
            sa.Uuid(),
            nullable=True,
            comment="Step that generated this message",
        ),
        sa.Column(
            "sequence_number",
            sa.Integer(),
            nullable=False,
            comment="Order in conversation",
        ),
        sa.Column(
            "role",
            postgresql.ENUM(
                "system",
                "user",
                "assistant",
                "tool",
                "function",
                name="aiagentmessagerole",
            ),
            nullable=False,
        ),
        sa.Column("content", sa.Text(), nullable=True, comment="Message text content"),
        sa.Column(
            "content_json",
            sa.JSON(),
            nullable=True,
            comment="Structured content (for multi-modal or tool results)",
        ),
        sa.Column(
            "function_name",
            sa.String(length=255),
            nullable=True,
            comment="Function name for function calls",
        ),
        sa.Column(
            "function_args", sa.JSON(), nullable=True, comment="Function arguments"
        ),
        sa.Column(
            "tool_call_id",
            sa.String(length=255),
            nullable=True,
            comment="Tool call ID for correlation",
        ),
        sa.Column(
            "token_count",
            sa.Integer(),
            nullable=True,
            comment="Token count for this message",
        ),
        sa.Column("metadata_json", sa.JSON(), nullable=False),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(
            ["run_id"],
            ["ai_agent_runs.id"],
            name=op.f("fk_ai_agent_messages_run_id_ai_agent_runs"),
            ondelete="CASCADE",
        ),
        sa.ForeignKeyConstraint(
            ["step_id"],
            ["ai_agent_steps.id"],
            name=op.f("fk_ai_agent_messages_step_id_ai_agent_steps"),
            ondelete="SET NULL",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_ai_agent_messages")),
    )
    with op.batch_alter_table("ai_agent_messages", schema=None) as batch_op:
        batch_op.create_index("ix_ai_agent_messages_run", ["run_id"], unique=False)
        batch_op.create_index(
            "ix_ai_agent_messages_run_seq", ["run_id", "sequence_number"], unique=False
        )

    op.create_table(
        "ai_agent_tool_calls",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("run_id", sa.Uuid(), nullable=False),
        sa.Column("step_id", sa.Uuid(), nullable=True),
        sa.Column(
            "tool_call_id",
            sa.String(length=255),
            nullable=False,
            comment="Unique tool call ID from LLM",
        ),
        sa.Column("tool_name", sa.String(length=255), nullable=False),
        sa.Column("tool_args", sa.JSON(), nullable=False),
        sa.Column("result", sa.JSON(), nullable=True, comment="Tool execution result"),
        sa.Column(
            "result_text",
            sa.Text(),
            nullable=True,
            comment="Text representation of result",
        ),
        sa.Column("success", sa.Boolean(), nullable=False),
        sa.Column("error_message", sa.Text(), nullable=True),
        sa.Column("started_at", sa.DateTime(), nullable=True),
        sa.Column("completed_at", sa.DateTime(), nullable=True),
        sa.Column("duration_ms", sa.Float(), nullable=True),
        sa.Column("metadata_json", sa.JSON(), nullable=False),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(
            ["run_id"],
            ["ai_agent_runs.id"],
            name=op.f("fk_ai_agent_tool_calls_run_id_ai_agent_runs"),
            ondelete="CASCADE",
        ),
        sa.ForeignKeyConstraint(
            ["step_id"],
            ["ai_agent_steps.id"],
            name=op.f("fk_ai_agent_tool_calls_step_id_ai_agent_steps"),
            ondelete="SET NULL",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_ai_agent_tool_calls")),
    )
    with op.batch_alter_table("ai_agent_tool_calls", schema=None) as batch_op:
        batch_op.create_index(
            "ix_ai_agent_tool_calls_call_id", ["tool_call_id"], unique=False
        )
        batch_op.create_index("ix_ai_agent_tool_calls_run", ["run_id"], unique=False)
        batch_op.create_index(
            "ix_ai_agent_tool_calls_tool", ["tool_name"], unique=False
        )

    op.create_table(
        "ai_workflow_approvals",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("workflow_execution_id", sa.Uuid(), nullable=False),
        sa.Column("node_name", sa.String(length=255), nullable=False),
        sa.Column("prompt", sa.Text(), nullable=False),
        sa.Column("options", sa.JSON(), nullable=False),
        sa.Column("context", sa.JSON(), nullable=False),
        sa.Column("is_pending", sa.Boolean(), nullable=False),
        sa.Column("timeout_seconds", sa.Integer(), nullable=True),
        sa.Column("response", sa.String(length=100), nullable=True),
        sa.Column("response_data", sa.JSON(), nullable=True),
        sa.Column("response_comment", sa.Text(), nullable=True),
        sa.Column("responded_by_id", sa.Integer(), nullable=True),
        sa.Column("responded_at", sa.DateTime(), nullable=True),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.Column("expires_at", sa.DateTime(), nullable=True),
        sa.Column("metadata", sa.JSON(), nullable=False),
        sa.ForeignKeyConstraint(
            ["responded_by_id"],
            ["users.id"],
            name=op.f("fk_ai_workflow_approvals_responded_by_id_users"),
            ondelete="SET NULL",
        ),
        sa.ForeignKeyConstraint(
            ["workflow_execution_id"],
            ["ai_workflow_executions.id"],
            name=op.f(
                "fk_ai_workflow_approvals_workflow_execution_id_ai_workflow_executions"
            ),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_ai_workflow_approvals")),
    )
    with op.batch_alter_table("ai_workflow_approvals", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_ai_workflow_approvals_is_pending"),
            ["is_pending"],
            unique=False,
        )
        batch_op.create_index(
            batch_op.f("ix_ai_workflow_approvals_workflow_execution_id"),
            ["workflow_execution_id"],
            unique=False,
        )
        batch_op.create_index(
            "ix_workflow_approvals_expires", ["is_pending", "expires_at"], unique=False
        )
        batch_op.create_index(
            "ix_workflow_approvals_pending",
            ["is_pending", "workflow_execution_id"],
            unique=False,
        )

    op.create_table(
        "ai_workflow_node_executions",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("workflow_execution_id", sa.Uuid(), nullable=False),
        sa.Column("node_name", sa.String(length=255), nullable=False),
        sa.Column(
            "node_type",
            postgresql.ENUM(
                "function",
                "human_approval",
                "conditional",
                "parallel",
                "subworkflow",
                name="aiworkflownodetype",
            ),
            nullable=False,
        ),
        sa.Column(
            "status",
            postgresql.ENUM(
                "pending",
                "running",
                "completed",
                "failed",
                "skipped",
                "waiting_approval",
                "approved",
                "rejected",
                name="aiworkflownodestatus",
            ),
            nullable=False,
        ),
        sa.Column("execution_order", sa.Integer(), nullable=False),
        sa.Column("input_data", sa.JSON(), nullable=False),
        sa.Column("output_data", sa.JSON(), nullable=True),
        sa.Column("error", sa.Text(), nullable=True),
        sa.Column("error_code", sa.String(length=100), nullable=True),
        sa.Column("started_at", sa.DateTime(), nullable=True),
        sa.Column("completed_at", sa.DateTime(), nullable=True),
        sa.Column("attempt_number", sa.Integer(), nullable=False),
        sa.Column("metadata", sa.JSON(), nullable=False),
        sa.ForeignKeyConstraint(
            ["workflow_execution_id"],
            ["ai_workflow_executions.id"],
            name=op.f(
                "fk_ai_workflow_node_executions_workflow_execution_id_ai_workflow_executions"
            ),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_ai_workflow_node_executions")),
    )
    with op.batch_alter_table("ai_workflow_node_executions", schema=None) as batch_op:
        batch_op.create_index(
            batch_op.f("ix_ai_workflow_node_executions_status"),
            ["status"],
            unique=False,
        )
        batch_op.create_index(
            batch_op.f("ix_ai_workflow_node_executions_workflow_execution_id"),
            ["workflow_execution_id"],
            unique=False,
        )
        batch_op.create_index(
            "ix_workflow_node_executions_workflow_node",
            ["workflow_execution_id", "node_name"],
            unique=False,
        )

    # ### end Alembic commands ###


def downgrade() -> None:  # noqa: PLR0915
    """Downgrade database schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    with op.batch_alter_table("ai_workflow_node_executions", schema=None) as batch_op:
        batch_op.drop_index("ix_workflow_node_executions_workflow_node")
        batch_op.drop_index(
            batch_op.f("ix_ai_workflow_node_executions_workflow_execution_id")
        )
        batch_op.drop_index(batch_op.f("ix_ai_workflow_node_executions_status"))

    op.drop_table("ai_workflow_node_executions")
    with op.batch_alter_table("ai_workflow_approvals", schema=None) as batch_op:
        batch_op.drop_index("ix_workflow_approvals_pending")
        batch_op.drop_index("ix_workflow_approvals_expires")
        batch_op.drop_index(
            batch_op.f("ix_ai_workflow_approvals_workflow_execution_id")
        )
        batch_op.drop_index(batch_op.f("ix_ai_workflow_approvals_is_pending"))

    op.drop_table("ai_workflow_approvals")
    with op.batch_alter_table("ai_agent_tool_calls", schema=None) as batch_op:
        batch_op.drop_index("ix_ai_agent_tool_calls_tool")
        batch_op.drop_index("ix_ai_agent_tool_calls_run")
        batch_op.drop_index("ix_ai_agent_tool_calls_call_id")

    op.drop_table("ai_agent_tool_calls")
    with op.batch_alter_table("ai_agent_messages", schema=None) as batch_op:
        batch_op.drop_index("ix_ai_agent_messages_run_seq")
        batch_op.drop_index("ix_ai_agent_messages_run")

    op.drop_table("ai_agent_messages")
    with op.batch_alter_table("ai_workflow_executions", schema=None) as batch_op:
        batch_op.drop_index("ix_workflow_executions_tenant_status")
        batch_op.drop_index("ix_workflow_executions_definition")
        batch_op.drop_index("ix_workflow_executions_created")
        batch_op.drop_index(batch_op.f("ix_ai_workflow_executions_tenant_id"))
        batch_op.drop_index(batch_op.f("ix_ai_workflow_executions_status"))
        batch_op.drop_index(batch_op.f("ix_ai_workflow_executions_definition_id"))

    op.drop_table("ai_workflow_executions")
    with op.batch_alter_table("ai_usage_logs", schema=None) as batch_op:
        batch_op.drop_index("ix_ai_usage_logs_tenant_provider")
        batch_op.drop_index("ix_ai_usage_logs_tenant_created")
        batch_op.drop_index("ix_ai_usage_logs_job")
        batch_op.drop_index("ix_ai_usage_logs_created_at")

    op.drop_table("ai_usage_logs")
    with op.batch_alter_table("ai_agent_steps", schema=None) as batch_op:
        batch_op.drop_index("ix_ai_agent_steps_status")
        batch_op.drop_index("ix_ai_agent_steps_run_step")
        batch_op.drop_index("ix_ai_agent_steps_run")

    op.drop_table("ai_agent_steps")
    with op.batch_alter_table("ai_agent_checkpoints", schema=None) as batch_op:
        batch_op.drop_index("ix_ai_agent_checkpoints_valid")
        batch_op.drop_index("ix_ai_agent_checkpoints_run_step")
        batch_op.drop_index("ix_ai_agent_checkpoints_run")

    op.drop_table("ai_agent_checkpoints")
    with op.batch_alter_table("webhook_deliveries", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_webhook_deliveries_webhook_id"))
        batch_op.drop_index(batch_op.f("ix_webhook_deliveries_status"))
        batch_op.drop_index(batch_op.f("ix_webhook_deliveries_next_retry_at"))
        batch_op.drop_index(batch_op.f("ix_webhook_deliveries_event_type"))
        batch_op.drop_index(batch_op.f("ix_webhook_deliveries_event_id"))

    op.drop_table("webhook_deliveries")
    op.drop_table("tenant_ai_features")
    with op.batch_alter_table("tenant_ai_configs", schema=None) as batch_op:
        batch_op.drop_index("ix_tenant_ai_configs_tenant_provider")
        batch_op.drop_index("ix_tenant_ai_configs_tenant_active")

    op.drop_table("tenant_ai_configs")
    op.drop_table("reminder_tags")
    with op.batch_alter_table("posts", schema=None) as batch_op:
        batch_op.drop_index("ix_posts_slug")
        batch_op.drop_index("ix_posts_author_id_is_published")
        batch_op.drop_index(batch_op.f("ix_posts_author_id"))

    op.drop_table("posts")
    with op.batch_alter_table("job_webhooks", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_job_webhooks_job_id"))

    op.drop_table("job_webhooks")
    op.drop_table("job_progress")
    with op.batch_alter_table("job_labels", schema=None) as batch_op:
        batch_op.drop_index("ix_job_labels_key_value")
        batch_op.drop_index(batch_op.f("ix_job_labels_job_id"))

    op.drop_table("job_labels")
    with op.batch_alter_table("job_dependencies", schema=None) as batch_op:
        batch_op.drop_index(
            "ix_job_deps_unsatisfied", postgresql_where=sa.text("NOT satisfied")
        )
        batch_op.drop_index(batch_op.f("ix_job_dependencies_job_id"))
        batch_op.drop_index(batch_op.f("ix_job_dependencies_depends_on_job_id"))

    op.drop_table("job_dependencies")
    with op.batch_alter_table("job_audit_logs", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_job_audit_logs_job_id"))
        batch_op.drop_index(batch_op.f("ix_job_audit_logs_created_at"))
        batch_op.drop_index("ix_job_audit_job_created")

    op.drop_table("job_audit_logs")
    with op.batch_alter_table("file_thumbnails", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_file_thumbnails_file_id"))

    op.drop_table("file_thumbnails")
    with op.batch_alter_table("email_usage_logs", schema=None) as batch_op:
        batch_op.drop_index("ix_email_usage_logs_tenant_provider")
        batch_op.drop_index(batch_op.f("ix_email_usage_logs_tenant_id"))
        batch_op.drop_index("ix_email_usage_logs_tenant_created")
        batch_op.drop_index("ix_email_usage_logs_created_at")

    op.drop_table("email_usage_logs")
    with op.batch_alter_table("email_configs", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_email_configs_tenant_id"))
        batch_op.drop_index("ix_email_configs_tenant_active")
        batch_op.drop_index("ix_email_configs_provider")

    op.drop_table("email_configs")
    with op.batch_alter_table("email_audit_logs", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_email_audit_logs_tenant_id"))
        batch_op.drop_index("ix_email_audit_logs_tenant_created")
        batch_op.drop_index("ix_email_audit_logs_status")
        batch_op.drop_index(batch_op.f("ix_email_audit_logs_recipient_hash"))
        batch_op.drop_index("ix_email_audit_logs_recipient")
        batch_op.drop_index(batch_op.f("ix_email_audit_logs_created_at"))

    op.drop_table("email_audit_logs")
    with op.batch_alter_table("ai_workflow_definitions", schema=None) as batch_op:
        batch_op.drop_index("ix_workflow_definitions_tenant_slug")
        batch_op.drop_index("ix_workflow_definitions_tenant_active")
        batch_op.drop_index(batch_op.f("ix_ai_workflow_definitions_tenant_id"))
        batch_op.drop_index(batch_op.f("ix_ai_workflow_definitions_slug"))

    op.drop_table("ai_workflow_definitions")
    with op.batch_alter_table("ai_jobs", schema=None) as batch_op:
        batch_op.drop_index("ix_ai_jobs_tenant_type")
        batch_op.drop_index("ix_ai_jobs_tenant_status")
        batch_op.drop_index("ix_ai_jobs_status_created")
        batch_op.drop_index("ix_ai_jobs_created_at")

    op.drop_table("ai_jobs")
    with op.batch_alter_table("ai_agent_runs", schema=None) as batch_op:
        batch_op.drop_index("ix_ai_agent_runs_tenant_status")
        batch_op.drop_index("ix_ai_agent_runs_tenant_agent")
        batch_op.drop_index("ix_ai_agent_runs_status_created")
        batch_op.drop_index("ix_ai_agent_runs_parent")
        batch_op.drop_index("ix_ai_agent_runs_created_at")

    op.drop_table("ai_agent_runs")
    with op.batch_alter_table("webhooks", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_webhooks_tenant_id"))

    op.drop_table("webhooks")
    with op.batch_alter_table("users", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_users_username"))
        batch_op.drop_index("ix_users_is_active")
        batch_op.drop_index("ix_users_email_username")
        batch_op.drop_index(batch_op.f("ix_users_email"))

    op.drop_table("users")
    op.drop_table("tenants")
    with op.batch_alter_table("task_executions", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_task_executions_worker_id"))
        batch_op.drop_index(batch_op.f("ix_task_executions_task_name"))
        batch_op.drop_index(batch_op.f("ix_task_executions_task_id"))
        batch_op.drop_index(batch_op.f("ix_task_executions_status"))
        batch_op.drop_index(batch_op.f("ix_task_executions_error_type"))
        batch_op.drop_index(batch_op.f("ix_task_executions_created_at"))
        batch_op.drop_index("ix_task_exec_worker_status")
        batch_op.drop_index("ix_task_exec_status_created")
        batch_op.drop_index("ix_task_exec_name_status")
        batch_op.drop_index("ix_task_exec_created_desc", postgresql_using="btree")

    op.drop_table("task_executions")
    with op.batch_alter_table("tags", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_tags_name"))

    op.drop_table("tags")
    with op.batch_alter_table("search_suggestion_logs", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_search_suggestion_logs_prefix"))

    op.drop_table("search_suggestion_logs")
    with op.batch_alter_table("search_query_profiles", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_search_query_profiles_query_type"))
        batch_op.drop_index(batch_op.f("ix_search_query_profiles_is_slow"))
        batch_op.drop_index(batch_op.f("ix_search_query_profiles_execution_time_ms"))

    op.drop_table("search_query_profiles")
    with op.batch_alter_table("search_queries", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_search_queries_user_id"))
        batch_op.drop_index(batch_op.f("ix_search_queries_query_text"))
        batch_op.drop_index(batch_op.f("ix_search_queries_query_hash"))

    op.drop_table("search_queries")
    with op.batch_alter_table("search_experiments", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_search_experiments_status"))
        batch_op.drop_index(batch_op.f("ix_search_experiments_name"))

    op.drop_table("search_experiments")
    with op.batch_alter_table("search_experiment_events", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_search_experiment_events_user_id"))
        batch_op.drop_index(batch_op.f("ix_search_experiment_events_experiment_name"))
        batch_op.drop_index(batch_op.f("ix_search_experiment_events_experiment_id"))
        batch_op.drop_index(batch_op.f("ix_search_experiment_events_event_type"))

    op.drop_table("search_experiment_events")
    with op.batch_alter_table("search_experiment_assignments", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_search_experiment_assignments_user_id"))
        batch_op.drop_index(
            batch_op.f("ix_search_experiment_assignments_experiment_name")
        )
        batch_op.drop_index(
            batch_op.f("ix_search_experiment_assignments_experiment_id")
        )

    op.drop_table("search_experiment_assignments")
    with op.batch_alter_table("reminders", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_reminders_tenant_id"))
        batch_op.drop_index(batch_op.f("ix_reminders_parent_id"))

    op.drop_table("reminders")
    with op.batch_alter_table("jobs", schema=None) as batch_op:
        batch_op.drop_index("ix_jobs_tenant_status")
        batch_op.drop_index(batch_op.f("ix_jobs_tenant_id"))
        batch_op.drop_index("ix_jobs_task_name_status")
        batch_op.drop_index(batch_op.f("ix_jobs_task_name"))
        batch_op.drop_index(batch_op.f("ix_jobs_status"))
        batch_op.drop_index(
            "ix_jobs_running_timeout",
            postgresql_where=sa.text(
                "status = 'running' AND timeout_seconds IS NOT NULL"
            ),
        )
        batch_op.drop_index(
            "ix_jobs_priority_queued", postgresql_where=sa.text("status = 'queued'")
        )
        batch_op.drop_index(batch_op.f("ix_jobs_parent_job_id"))
        batch_op.drop_index(
            "ix_jobs_completed_cleanup",
            postgresql_where=sa.text("status IN ('completed', 'failed', 'cancelled')"),
        )

    op.drop_table("jobs")
    with op.batch_alter_table("flag_overrides", schema=None) as batch_op:
        batch_op.drop_index("ix_flag_overrides_lookup")
        batch_op.drop_index(batch_op.f("ix_flag_overrides_flag_key"))
        batch_op.drop_index("ix_flag_overrides_entity")

    op.drop_table("flag_overrides")
    with op.batch_alter_table("files", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_files_tenant_id"))
        batch_op.drop_index(batch_op.f("ix_files_storage_key"))
        batch_op.drop_index(batch_op.f("ix_files_status"))
        batch_op.drop_index(batch_op.f("ix_files_owner_id"))

    op.drop_table("files")
    with op.batch_alter_table("feature_flags", schema=None) as batch_op:
        batch_op.drop_index("ix_feature_flags_status")
        batch_op.drop_index(batch_op.f("ix_feature_flags_key"))
        batch_op.drop_index("ix_feature_flags_enabled")

    op.drop_table("feature_flags")
    with op.batch_alter_table("event_outbox", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_event_outbox_processed_at"))
        batch_op.drop_index(
            "ix_event_outbox_pending", postgresql_where=sa.text("processed_at IS NULL")
        )
        batch_op.drop_index(batch_op.f("ix_event_outbox_next_retry_at"))
        batch_op.drop_index(batch_op.f("ix_event_outbox_event_type"))
        batch_op.drop_index(batch_op.f("ix_event_outbox_correlation_id"))
        batch_op.drop_index("ix_event_outbox_aggregate")

    op.drop_table("event_outbox")
    with op.batch_alter_table("audit_logs", schema=None) as batch_op:
        batch_op.drop_index("ix_audit_user_time")
        batch_op.drop_index("ix_audit_tenant_user_time")
        batch_op.drop_index("ix_audit_tenant_time")
        batch_op.drop_index(batch_op.f("ix_audit_logs_user_id"))
        batch_op.drop_index(batch_op.f("ix_audit_logs_timestamp"))
        batch_op.drop_index(batch_op.f("ix_audit_logs_tenant_id"))
        batch_op.drop_index(batch_op.f("ix_audit_logs_request_id"))
        batch_op.drop_index(batch_op.f("ix_audit_logs_entity_type"))
        batch_op.drop_index(batch_op.f("ix_audit_logs_entity_id"))
        batch_op.drop_index(batch_op.f("ix_audit_logs_action"))
        batch_op.drop_index("ix_audit_entity")
        batch_op.drop_index("ix_audit_action_time")
        batch_op.drop_index("ix_audit_action_entity")

    op.drop_table("audit_logs")
    # ### end Alembic commands ###
